Terminal execution:  True   (sys.argv[0]:  analysis-transformers-run.py )
Arguments passed via the terminal:

cap-merge    dataset
500    sample_size_train
None    sample_size_corpus
20000    sample_size_no_topic
None    sample_size_test
nli_short    method
MoritzLaurer/deberta-v3-base-zeroshot-v1    model
transformer    vectorizer
20231026    study_date
cap-merge    task
1    n_run
6    n_random_runs_total
random1    group_sample
True    save_outputs
domain    group_column
448    max_length
Iteration number:  0
All random seeds:  [7270  860 5390 5191 5734 6265]
Random seed for this run:  7270
deberta-v3-base-zeroshot-v
Dataset:  cap-merge 

Overall label distribution per group member:
 domain                       
legal   Law and Crime            1949
        Civil Rights              912
        Domestic Commerce         786
        Labor                     580
        Government Operations     447
speech  Government Operations     887
        Law and Crime             697
        Labor                     679
        Civil Rights              466
        Domestic Commerce         305
Name: label_text, dtype: int64
Group selected: ['speech']  for seed 7270
Sample that might be imbalanced: df_train.label_text.value_counts:
 Civil Rights             100
Domestic Commerce        100
Government Operations    100
Labor                    100
Law and Crime            100
Name: label_text, dtype: int64

FINAL DF_TRAIN SAMPLE (BALANCED) for group \bspeech\b:
df_train.label_text.value_counts:
 Civil Rights             100
Domestic Commerce        100
Government Operations    100
Labor                    100
Law and Crime            100
Name: label_text, dtype: int64

For NLI: Augmenting data by adding random not_entail examples to the train set from other classes within the train set.
Length of df_train before this step is: 500.

Max augmentation can be: len(df_train) * 2 = 1000. Can also be lower, if there are more entail examples than not-entail for a majority class
For NLI:  not_entail training examples were added, which leads to an augmented training dataset of length 1000.
Number of hypotheses/classes:  5 

For normal test, N classifications necessary: 1928
For NLI test, N classifications necessary: 9640

Downloading (…)okenizer_config.json:   0%|          | 0.00/492 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 492/492 [00:00<00:00, 187kB/s]
Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]Downloading spm.model: 100%|██████████| 2.46M/2.46M [00:00<00:00, 4.44MB/s]Downloading spm.model: 100%|██████████| 2.46M/2.46M [00:00<00:00, 4.43MB/s]
Downloading (…)/main/tokenizer.json:   0%|          | 0.00/8.65M [00:00<?, ?B/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 8.65M/8.65M [00:00<00:00, 14.9MB/s]Downloading (…)/main/tokenizer.json: 100%|██████████| 8.65M/8.65M [00:00<00:00, 14.8MB/s]
Downloading (…)in/added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]Downloading (…)in/added_tokens.json: 100%|██████████| 23.0/23.0 [00:00<00:00, 8.08kB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 173/173 [00:00<00:00, 220kB/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.07k/1.07k [00:00<00:00, 1.05MB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/369M [00:00<?, ?B/s]Downloading pytorch_model.bin:   6%|▌         | 21.0M/369M [00:00<00:01, 180MB/s]Downloading pytorch_model.bin:  14%|█▍        | 52.4M/369M [00:00<00:01, 202MB/s]Downloading pytorch_model.bin:  23%|██▎       | 83.9M/369M [00:00<00:01, 237MB/s]Downloading pytorch_model.bin:  34%|███▍      | 126M/369M [00:00<00:00, 272MB/s] Downloading pytorch_model.bin:  43%|████▎     | 157M/369M [00:00<00:00, 283MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 199M/369M [00:00<00:00, 306MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 231M/369M [00:00<00:00, 291MB/s]Downloading pytorch_model.bin:  71%|███████   | 262M/369M [00:00<00:00, 286MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 304M/369M [00:01<00:00, 286MB/s]Downloading pytorch_model.bin:  91%|█████████ | 336M/369M [00:01<00:00, 286MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 367M/369M [00:01<00:00, 275MB/s]Downloading pytorch_model.bin: 100%|██████████| 369M/369M [00:01<00:00, 271MB/s]
Device: cuda
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]                                                    Map:   0%|          | 0/9640 [00:00<?, ? examples/s]Map:  10%|█         | 1000/9640 [00:00<00:01, 6307.56 examples/s]Map:  21%|██        | 2000/9640 [00:00<00:01, 6312.49 examples/s]Map:  31%|███       | 3000/9640 [00:00<00:01, 6092.91 examples/s]Map:  41%|████▏     | 4000/9640 [00:00<00:00, 6010.36 examples/s]Map:  52%|█████▏    | 5000/9640 [00:00<00:00, 6225.53 examples/s]Map:  62%|██████▏   | 6000/9640 [00:00<00:00, 5911.61 examples/s]Map:  73%|███████▎  | 7000/9640 [00:01<00:00, 6103.86 examples/s]Map:  83%|████████▎ | 8000/9640 [00:01<00:00, 6246.30 examples/s]Map:  93%|█████████▎| 9000/9640 [00:01<00:00, 6197.25 examples/s]Map: 100%|██████████| 9640/9640 [00:01<00:00, 6220.36 examples/s]                                                                   0%|          | 0/224 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/224 [00:00<01:04,  3.47it/s]  1%|▏         | 3/224 [00:00<00:30,  7.32it/s]  2%|▏         | 5/224 [00:00<00:24,  9.00it/s]  3%|▎         | 7/224 [00:00<00:22,  9.86it/s]  4%|▍         | 9/224 [00:00<00:20, 10.48it/s]  5%|▍         | 11/224 [00:01<00:19, 10.88it/s]  6%|▌         | 13/224 [00:01<00:19, 11.01it/s]  7%|▋         | 15/224 [00:01<00:18, 11.17it/s]  8%|▊         | 17/224 [00:01<00:18, 11.28it/s]  8%|▊         | 19/224 [00:01<00:18, 11.37it/s]  9%|▉         | 21/224 [00:02<00:17, 11.49it/s] 10%|█         | 23/224 [00:02<00:17, 11.43it/s] 11%|█         | 25/224 [00:02<00:17, 11.45it/s] 12%|█▏        | 27/224 [00:02<00:17, 11.54it/s] 13%|█▎        | 29/224 [00:02<00:16, 11.52it/s] 14%|█▍        | 31/224 [00:02<00:16, 11.60it/s]                                                 14%|█▍        | 32/224 [00:02<00:16, 11.60it/s] 15%|█▍        | 33/224 [00:03<00:16, 11.38it/s] 16%|█▌        | 35/224 [00:03<00:16, 11.47it/s] 17%|█▋        | 37/224 [00:03<00:16, 11.49it/s] 17%|█▋        | 39/224 [00:03<00:16, 11.54it/s] 18%|█▊        | 41/224 [00:03<00:15, 11.55it/s] 19%|█▉        | 43/224 [00:03<00:15, 11.59it/s] 20%|██        | 45/224 [00:04<00:15, 11.64it/s] 21%|██        | 47/224 [00:04<00:15, 11.62it/s] 22%|██▏       | 49/224 [00:04<00:15, 11.56it/s] 23%|██▎       | 51/224 [00:04<00:15, 11.53it/s] 24%|██▎       | 53/224 [00:04<00:14, 11.45it/s] 25%|██▍       | 55/224 [00:04<00:14, 11.50it/s] 25%|██▌       | 57/224 [00:05<00:14, 11.56it/s] 26%|██▋       | 59/224 [00:05<00:14, 11.56it/s] 27%|██▋       | 61/224 [00:05<00:14, 11.55it/s] 28%|██▊       | 63/224 [00:05<00:13, 11.57it/s]                                                 29%|██▊       | 64/224 [00:05<00:13, 11.57it/s] 29%|██▉       | 65/224 [00:05<00:13, 11.73it/s] 30%|██▉       | 67/224 [00:05<00:13, 11.75it/s] 31%|███       | 69/224 [00:06<00:13, 11.67it/s] 32%|███▏      | 71/224 [00:06<00:13, 11.70it/s] 33%|███▎      | 73/224 [00:06<00:12, 11.72it/s] 33%|███▎      | 75/224 [00:06<00:12, 11.61it/s] 34%|███▍      | 77/224 [00:06<00:12, 11.65it/s] 35%|███▌      | 79/224 [00:07<00:12, 11.64it/s] 36%|███▌      | 81/224 [00:07<00:12, 11.61it/s] 37%|███▋      | 83/224 [00:07<00:12, 11.62it/s] 38%|███▊      | 85/224 [00:07<00:12, 11.58it/s] 39%|███▉      | 87/224 [00:07<00:11, 11.57it/s] 40%|███▉      | 89/224 [00:07<00:11, 11.60it/s] 41%|████      | 91/224 [00:08<00:11, 11.63it/s] 42%|████▏     | 93/224 [00:08<00:11, 11.57it/s] 42%|████▏     | 95/224 [00:08<00:11, 11.55it/s]                                                 43%|████▎     | 96/224 [00:08<00:11, 11.55it/s] 43%|████▎     | 97/224 [00:08<00:10, 11.57it/s] 44%|████▍     | 99/224 [00:08<00:10, 11.58it/s] 45%|████▌     | 101/224 [00:08<00:10, 11.62it/s] 46%|████▌     | 103/224 [00:09<00:10, 11.63it/s] 47%|████▋     | 105/224 [00:09<00:10, 11.62it/s] 48%|████▊     | 107/224 [00:09<00:10, 11.65it/s] 49%|████▊     | 109/224 [00:09<00:09, 11.68it/s] 50%|████▉     | 111/224 [00:09<00:09, 11.62it/s] 50%|█████     | 113/224 [00:09<00:09, 11.61it/s] 51%|█████▏    | 115/224 [00:10<00:09, 11.57it/s] 52%|█████▏    | 117/224 [00:10<00:09, 11.53it/s] 53%|█████▎    | 119/224 [00:10<00:09, 11.54it/s] 54%|█████▍    | 121/224 [00:10<00:08, 11.55it/s] 55%|█████▍    | 123/224 [00:10<00:08, 11.50it/s] 56%|█████▌    | 125/224 [00:11<00:08, 11.43it/s] 57%|█████▋    | 127/224 [00:11<00:08, 11.47it/s]                                                  57%|█████▋    | 128/224 [00:11<00:08, 11.47it/s] 58%|█████▊    | 129/224 [00:11<00:08, 11.61it/s] 58%|█████▊    | 131/224 [00:11<00:08, 11.56it/s] 59%|█████▉    | 133/224 [00:11<00:07, 11.53it/s] 60%|██████    | 135/224 [00:11<00:07, 11.55it/s] 61%|██████    | 137/224 [00:12<00:07, 11.57it/s] 62%|██████▏   | 139/224 [00:12<00:07, 11.62it/s] 63%|██████▎   | 141/224 [00:12<00:07, 11.57it/s] 64%|██████▍   | 143/224 [00:12<00:07, 11.55it/s] 65%|██████▍   | 145/224 [00:12<00:06, 11.52it/s] 66%|██████▌   | 147/224 [00:12<00:06, 11.55it/s] 67%|██████▋   | 149/224 [00:13<00:06, 11.52it/s] 67%|██████▋   | 151/224 [00:13<00:06, 11.55it/s] 68%|██████▊   | 153/224 [00:13<00:06, 11.60it/s] 69%|██████▉   | 155/224 [00:13<00:05, 11.63it/s] 70%|███████   | 157/224 [00:13<00:05, 11.65it/s] 71%|███████   | 159/224 [00:13<00:05, 11.64it/s]                                                  71%|███████▏  | 160/224 [00:14<00:05, 11.64it/s] 72%|███████▏  | 161/224 [00:14<00:05, 11.75it/s] 73%|███████▎  | 163/224 [00:14<00:05, 11.67it/s] 74%|███████▎  | 165/224 [00:14<00:05, 11.67it/s] 75%|███████▍  | 167/224 [00:14<00:04, 11.67it/s] 75%|███████▌  | 169/224 [00:14<00:04, 11.63it/s] 76%|███████▋  | 171/224 [00:14<00:04, 11.63it/s] 77%|███████▋  | 173/224 [00:15<00:04, 11.52it/s] 78%|███████▊  | 175/224 [00:15<00:04, 11.49it/s] 79%|███████▉  | 177/224 [00:15<00:04, 11.52it/s] 80%|███████▉  | 179/224 [00:15<00:03, 11.55it/s] 81%|████████  | 181/224 [00:15<00:03, 11.54it/s] 82%|████████▏ | 183/224 [00:16<00:03, 11.60it/s] 83%|████████▎ | 185/224 [00:16<00:03, 11.64it/s] 83%|████████▎ | 187/224 [00:16<00:03, 11.59it/s] 84%|████████▍ | 189/224 [00:16<00:03, 11.62it/s] 85%|████████▌ | 191/224 [00:16<00:02, 11.58it/s]                                                  86%|████████▌ | 192/224 [00:16<00:02, 11.58it/s] 86%|████████▌ | 193/224 [00:16<00:02, 11.71it/s] 87%|████████▋ | 195/224 [00:17<00:02, 11.68it/s] 88%|████████▊ | 197/224 [00:17<00:02, 11.68it/s] 89%|████████▉ | 199/224 [00:17<00:02, 11.62it/s] 90%|████████▉ | 201/224 [00:17<00:01, 11.60it/s] 91%|█████████ | 203/224 [00:17<00:01, 11.63it/s] 92%|█████████▏| 205/224 [00:17<00:01, 11.57it/s] 92%|█████████▏| 207/224 [00:18<00:01, 11.61it/s] 93%|█████████▎| 209/224 [00:18<00:01, 11.49it/s] 94%|█████████▍| 211/224 [00:18<00:01, 11.46it/s] 95%|█████████▌| 213/224 [00:18<00:00, 11.50it/s] 96%|█████████▌| 215/224 [00:18<00:00, 11.48it/s] 97%|█████████▋| 217/224 [00:18<00:00, 11.52it/s] 98%|█████████▊| 219/224 [00:19<00:00, 11.57it/s] 99%|█████████▊| 221/224 [00:19<00:00, 11.57it/s]100%|█████████▉| 223/224 [00:19<00:00, 11.57it/s]                                                 100%|██████████| 224/224 [00:19<00:00, 11.57it/s]                                                 100%|██████████| 224/224 [00:19<00:00, 11.57it/s]100%|██████████| 224/224 [00:19<00:00, 11.47it/s]
{'loss': 0.8195, 'learning_rate': 1.377777777777778e-05, 'epoch': 1.0}
{'loss': 0.3925, 'learning_rate': 1.798882681564246e-05, 'epoch': 2.0}
{'loss': 0.2591, 'learning_rate': 1.4413407821229052e-05, 'epoch': 3.0}
{'loss': 0.1866, 'learning_rate': 1.0837988826815644e-05, 'epoch': 4.0}
{'loss': 0.1084, 'learning_rate': 7.262569832402235e-06, 'epoch': 5.0}
{'loss': 0.0658, 'learning_rate': 3.687150837988827e-06, 'epoch': 6.0}
{'loss': 0.045, 'learning_rate': 1.11731843575419e-07, 'epoch': 7.0}
{'train_runtime': 19.5371, 'train_samples_per_second': 358.292, 'train_steps_per_second': 11.465, 'train_loss': 0.26814201048442293, 'epoch': 7.0}

Train time: 19.636242389678955 

  0%|          | 0/38 [00:00<?, ?it/s]  5%|▌         | 2/38 [00:00<00:16,  2.17it/s]  8%|▊         | 3/38 [00:01<00:22,  1.54it/s] 11%|█         | 4/38 [00:02<00:25,  1.34it/s] 13%|█▎        | 5/38 [00:03<00:26,  1.23it/s] 16%|█▌        | 6/38 [00:04<00:27,  1.17it/s] 18%|█▊        | 7/38 [00:05<00:27,  1.15it/s] 21%|██        | 8/38 [00:06<00:26,  1.12it/s] 24%|██▎       | 9/38 [00:07<00:26,  1.11it/s] 26%|██▋       | 10/38 [00:08<00:25,  1.10it/s] 29%|██▉       | 11/38 [00:09<00:24,  1.10it/s] 32%|███▏      | 12/38 [00:10<00:23,  1.09it/s] 34%|███▍      | 13/38 [00:11<00:22,  1.09it/s] 37%|███▋      | 14/38 [00:11<00:22,  1.09it/s] 39%|███▉      | 15/38 [00:12<00:21,  1.09it/s] 42%|████▏     | 16/38 [00:13<00:20,  1.09it/s] 45%|████▍     | 17/38 [00:14<00:19,  1.09it/s] 47%|████▋     | 18/38 [00:15<00:18,  1.09it/s] 50%|█████     | 19/38 [00:16<00:17,  1.09it/s] 53%|█████▎    | 20/38 [00:17<00:16,  1.09it/s] 55%|█████▌    | 21/38 [00:18<00:15,  1.09it/s] 58%|█████▊    | 22/38 [00:19<00:14,  1.08it/s] 61%|██████    | 23/38 [00:20<00:13,  1.08it/s] 63%|██████▎   | 24/38 [00:21<00:12,  1.08it/s] 66%|██████▌   | 25/38 [00:22<00:11,  1.09it/s] 68%|██████▊   | 26/38 [00:23<00:11,  1.09it/s] 71%|███████   | 27/38 [00:23<00:10,  1.09it/s] 74%|███████▎  | 28/38 [00:24<00:09,  1.09it/s] 76%|███████▋  | 29/38 [00:25<00:08,  1.09it/s] 79%|███████▉  | 30/38 [00:26<00:07,  1.09it/s] 82%|████████▏ | 31/38 [00:27<00:06,  1.09it/s] 84%|████████▍ | 32/38 [00:28<00:05,  1.09it/s] 87%|████████▋ | 33/38 [00:29<00:04,  1.09it/s] 89%|████████▉ | 34/38 [00:30<00:03,  1.09it/s] 92%|█████████▏| 35/38 [00:31<00:02,  1.09it/s] 95%|█████████▍| 36/38 [00:32<00:01,  1.09it/s] 97%|█████████▋| 37/38 [00:33<00:00,  1.08it/s]100%|██████████| 38/38 [00:33<00:00,  1.21it/s]100%|██████████| 38/38 [00:33<00:00,  1.12it/s]
Highest probability prediction per premise:  [4, 0, 4, 3, 0, 4, 4, 2, 2, 2, 3, 1, 0, 4, 1, 1, 2, 1, 2, 1]
Correct labels per premise:  [2, 4, 0, 1, 0, 4, 4, 4, 0, 2, 4, 2, 2, 4, 1, 1, 0, 4, 2, 1]
Aggregate metrics:  {'f1_macro': 0.7101355850350239, 'f1_micro': 0.7282157676348547, 'accuracy_balanced': 0.717944181539601, 'accuracy_not_b': 0.7282157676348547, 'precision_macro': 0.7180986808486389, 'recall_macro': 0.717944181539601, 'precision_micro': 0.7282157676348547, 'recall_micro': 0.7282157676348547}
Detailed metrics:  {'Civil Rights': {'precision': 0.6768707482993197, 'recall': 0.5768115942028985, 'f1-score': 0.622848200312989, 'support': 345}, 'Domestic Commerce': {'precision': 0.6096256684491979, 'recall': 0.8351648351648352, 'f1-score': 0.7047913446676971, 'support': 273}, 'Government Operations': {'precision': 0.7520661157024794, 'recall': 0.5449101796407185, 'f1-score': 0.6319444444444445, 'support': 334}, 'Labor': {'precision': 0.7610619469026548, 'recall': 0.821656050955414, 'f1-score': 0.7901990811638592, 'support': 314}, 'Law and Crime': {'precision': 0.7908689248895434, 'recall': 0.8111782477341389, 'f1-score': 0.8008948545861297, 'support': 662}, 'accuracy': 0.7282157676348547, 'macro avg': {'precision': 0.7180986808486389, 'recall': 0.717944181539601, 'f1-score': 0.7101355850350239, 'support': 1928}, 'weighted avg': {'precision': 0.7332297603209729, 'recall': 0.7282157676348547, 'f1-score': 0.7244165020063157, 'support': 1928}} 


Test results:
{'eval_loss': 0.8516941666603088, 'eval_f1_macro': 0.7101355850350239, 'eval_f1_micro': 0.7282157676348547, 'eval_accuracy_balanced': 0.717944181539601, 'eval_accuracy_not_b': 0.7282157676348547, 'eval_precision_macro': 0.7180986808486389, 'eval_recall_macro': 0.717944181539601, 'eval_precision_micro': 0.7282157676348547, 'eval_recall_micro': 0.7282157676348547, 'eval_runtime': 34.7246, 'eval_samples_per_second': 277.613, 'eval_steps_per_second': 1.094, 'epoch': 7.0}

Script done.


